{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\n\nimport pandas as pd\npd.options.display.max_columns = 100\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nimport pylab as plot\nparams = { \n    'axes.labelsize': \"large\",\n    'xtick.labelsize': 'x-large',\n    'legend.fontsize': 20,\n    'figure.dpi': 150,\n    'figure.figsize': [25, 7]\n}\nplot.rcParams.update(params)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#tweak the style of this notebook a little bit to have centered plots\nfrom IPython.core.display import HTML\nHTML(\"\"\"\n<style>\n.output_png {\n    display: table-cell;\n    text-align: center;\n    vertical-align: middle;\n}\n</style>\n\"\"\");",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "acbee2d56d0f992bb29ed57d316edae1bc1d7504"
      },
      "cell_type": "code",
      "source": "data = pd.read_csv('../input/train.csv')\n\n#this will give us our insight into out data (rows, columns)\nprint(data.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "06fe041e1e72ab17030315fed4d7584f37dcda8a"
      },
      "cell_type": "code",
      "source": "#get a closer look at the data\ndata.head()\n\n#The Survived column is the target variable. If Suvival = 1 the passenger survived, otherwise they're dead. The is the variable we're going to predict.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ad505041a7bbfa3fc8b7689c4f23855771a019e8"
      },
      "cell_type": "code",
      "source": "# Pandas allows you to a have a high-level simple statistical description of the numerical features. This can be done using the describe method.\ndata.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e96c7bb85698644ce59a3dd5fca2746e45f5dec0"
      },
      "cell_type": "code",
      "source": "# The count variable shows that 177 values are missing in the Age column.\n# One solution is to fill in the null values with the median age. We could also impute with the mean age but the median is more robust to outliers.\n\ndata['Age'] = data['Age'].fillna(data['Age'].median())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5ad7747a05c7a771898dcd7ed0e059077c3b8537"
      },
      "cell_type": "code",
      "source": "# lets see what that does to the Age column\ndata.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b4bfbba8bff0cb20191d5f426b8b63963b55b7a3"
      },
      "cell_type": "markdown",
      "source": "**Time for some visualizations** "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a4f1f9e92ee5f554585e997b684bd8162ca182cf"
      },
      "cell_type": "code",
      "source": "data['Died'] = 1 - data['Survived']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8ccfe4a25244c2dbd9041f3b5623173eebff1f0d"
      },
      "cell_type": "code",
      "source": "data.groupby('Sex').agg('sum')[['Survived', 'Died']].plot(kind='bar', figsize=(25, 7),\n                                                          stacked=True, colors=['g', 'r']);",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cda32a45ebe3236d0c299a6719656a2c5ea6dd7a"
      },
      "cell_type": "code",
      "source": "#same graph but with ratio instead (mean)\ndata.groupby('Sex').agg('mean')[['Survived', 'Died']].plot(kind='bar', figsize=(25, 7), \n                                                           stacked=True, colors=['g', 'r']);",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f7dfd25859764bffe3e98a57154597ffe562aa91"
      },
      "cell_type": "code",
      "source": "# Let's now correlate the survival with the age variable.\nfig = plt.figure(figsize=(25, 7))\nsns.violinplot(x='Sex', y='Age', \n               hue='Survived', data=data, \n               split=True,\n               palette={0: \"r\", 1: \"g\"}\n              );\n\n# Women survive more than men, as depicted by the larger female green histogram below\n# Age is also a factor\n# Women and children first !",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "360e2e4dd0458666b46a7ed39a6bf9818cb2add2"
      },
      "cell_type": "code",
      "source": "# Let's now focus on the Fare ticket of each passenger and see how it could impact the survival.\n\nfigure = plt.figure(figsize=(25, 7))\nplt.hist([data[data['Survived'] == 1]['Fare'], data[data['Survived'] == 0]['Fare']], \n         stacked=True, color = ['g','r'],\n         bins = 50, label = ['Survived','Dead'])\nplt.xlabel('Fare')\nplt.ylabel('Number of passengers')\nplt.legend();\n\n# Passengers with cheaper ticket fares are more likely to die. \n# Put differently, passengers with more expensive tickets, and therefore a more important social status, seem to be rescued first.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1e385bce98cf49e617c659c4d73c241b54aa8a0b"
      },
      "cell_type": "code",
      "source": "# Ok this is nice. Let's now combine the age, the fare and the survival on a single chart.\n\nplt.figure(figsize=(25, 7))\nax = plt.subplot()\n\nax.scatter(data[data['Survived'] == 1]['Age'], data[data['Survived'] == 1]['Fare'], \n           c='green', s=data[data['Survived'] == 1]['Fare'])\nax.scatter(data[data['Survived'] == 0]['Age'], data[data['Survived'] == 0]['Fare'], \n           c='red', s=data[data['Survived'] == 0]['Fare']);\n\n# The size of the circles is proportional to the ticket fare.\n# On the x-axis, we have the ages and the y-axis, we consider the ticket fare.\n# We can observe different clusters:\n# - Large green dots between x=20 and x=45: adults with the largest ticket fares\n# - Small red dots between x=10 and x=45, adults from lower classes on the boat\n# - Small greed dots between x=0 and x=7: these are the children that were saved\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a84d1bd06220619c8342a2344e52593448e176af"
      },
      "cell_type": "code",
      "source": "# As a matter of fact, the ticket fare correlates with the class as we see it in the chart below.\n\nax = plt.subplot()\nax.set_ylabel('Average fare')\ndata.groupby('Pclass').mean()['Fare'].plot(kind='bar', figsize=(25, 7), ax = ax);",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "97e4e91e839ede9a4037b8c88d6581f55a5021f5"
      },
      "cell_type": "code",
      "source": "# how does the embarkation site affects the survival.\n\nfig = plt.figure(figsize=(25, 7))\nsns.violinplot(x='Embarked', y='Fare', hue='Survived', data=data, split=True, palette={0: \"r\", 1: \"g\"});\n\n# It seems that the embarkation C have a wider range of fare tickets and therefore the passengers who pay the highest prices are those who survive.\n# We also see this happening in embarkation S and less in embarkation Q.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e28c9d90008c7917ef70b66a364b545a9806d498"
      },
      "cell_type": "markdown",
      "source": "**Feature engineering**\n\nIn the previous part, we flirted with the data and spotted some interesting correlations.\n\nIn this part, we'll see how to process and transform these variables in such a way the data becomes manageable by a machine learning algorithm.\n\nWe'll also create, or \"engineer\" additional features that will be useful in building the model.\n\nWe'll see along the way how to process text variables like the passenger names and integrate this information in our model.\n\nWe will break our code in separate functions for more clarity."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "749649cd4902beaba130f87acd34da9af4120e25"
      },
      "cell_type": "code",
      "source": "# define a print function that asserts whether or not a feature has been processed.\n\ndef status(feature):\n    print('Processing', feature, ': ok')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f0649b6a27da561e84f172beaf0acc24a52bc120"
      },
      "cell_type": "markdown",
      "source": "\n**Loading the data**\n\nOne trick when starting a machine learning problem is to append the training set to the test set together.\n\nWe'll engineer new features using the train set to prevent information leakage. Then we'll add these variables to the test set.\n\nLet's load the train and test sets and append them together."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "097f828770715f3c231874b26b5ff6bb4dddeb8b"
      },
      "cell_type": "code",
      "source": "def get_combined_data():\n    # reading train data\n    train = pd.read_csv('../input/train.csv')\n    \n    # reading test data\n    test = pd.read_csv('../input/test.csv')\n\n    # extracting and then removing the targets from the training data \n    targets = train.Survived\n    train.drop(['Survived'], 1, inplace=True)\n    \n    # merging train data and test data for future feature engineering\n    # we'll also remove the PassengerID since this is not an informative feature\n    combined = train.append(test)\n    combined.reset_index(inplace=True)\n    combined.drop(['index', 'PassengerId'], inplace=True, axis=1)\n    \n    return combined",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "71370260029c24e94392c6d41a2a6912bd39ccf6"
      },
      "cell_type": "code",
      "source": "combined = get_combined_data()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "39510e3f6c7784ea3511533ae450eac766345a88"
      },
      "cell_type": "code",
      "source": "print(combined.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3d797c916ce06291f74c40b089c4052df5cdfa3d"
      },
      "cell_type": "code",
      "source": "combined.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c30cfef1e45d034b0a7ab692ba919a6954186721"
      },
      "cell_type": "markdown",
      "source": "\n**Extracting the passenger titles**\n\nWhen looking at the passenger names one could wonder how to process them to extract a useful information.\n\nIf you look closely at these first examples:\n\n    Braund, Mr. Owen Harris\n    Heikkinen, Miss. Laina\n    Oliva y Ocana, Dona. Fermina\n    Peter, Master. Michael J\n\nYou will notice that each name has a title in it ! This can be a simple Miss. or Mrs. but it can be sometimes something more sophisticated like Master, Sir or Dona. In that case, we might introduce an additional information about the social status by simply parsing the name and extracting the title and converting to a binary variable.\n\nLet's see how we'll do that in a function."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "97fc2a6e1843167c6762f16bcc4d2ffd25b63331"
      },
      "cell_type": "code",
      "source": "# Let's first see what the different titles are in the train set\n\ntitles = set()\nfor name in data['Name']:\n    titles.add(name.split(',')[1].split('.')[0].strip())\n\nprint(titles)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2ea3d8e9411f8d3520ccb22ee78d5e909e839688"
      },
      "cell_type": "code",
      "source": "Title_Dictionary = {\n    \"Capt\": \"Officer\",\n    \"Col\": \"Officer\",\n    \"Major\": \"Officer\",\n    \"Jonkheer\": \"Royalty\",\n    \"Don\": \"Royalty\",\n    \"Sir\" : \"Royalty\",\n    \"Dr\": \"Officer\",\n    \"Rev\": \"Officer\",\n    \"the Countess\":\"Royalty\",\n    \"Mme\": \"Mrs\",\n    \"Mlle\": \"Miss\",\n    \"Ms\": \"Mrs\",\n    \"Mr\" : \"Mr\",\n    \"Mrs\" : \"Mrs\",\n    \"Miss\" : \"Miss\",\n    \"Master\" : \"Master\",\n    \"Lady\" : \"Royalty\"\n}\n\ndef get_titles():\n    # we extract the title from each name\n    combined['Title'] = combined['Name'].map(lambda name:name.split(',')[1].split('.')[0].strip())\n    \n    # a map of more aggregated title\n    # we map each title\n    combined['Title'] = combined.Title.map(Title_Dictionary)\n    status('Title')\n    return combined",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7aba035175ee541b31fcbfef21d3faed080ea749"
      },
      "cell_type": "markdown",
      "source": "This function parses the names and extract the titles. Then, it maps the titles to categories of titles. We selected :\n\n    Officer\n    Royalty\n    Mr\n    Mrs\n    Miss\n    Master"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f8b5ea7a160530355850a77d2585242d18b4e102"
      },
      "cell_type": "code",
      "source": "combined = get_titles()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0eafc9b1cd3293b0a73505511f1763315e962d60"
      },
      "cell_type": "code",
      "source": "combined.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dd249cc3f97f8e332a7c553810671ec93e8527f1"
      },
      "cell_type": "code",
      "source": "# Let's check if the titles have been filled correctly.\n\ncombined[combined['Title'].isnull()]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4a5d5d019f1d33a09892f01a78f1693c41204006"
      },
      "cell_type": "markdown",
      "source": "**Processing the ages**\n\nWe have seen in the first part that the Age variable was missing 177 values. This is a large number ( ~ 13% of the dataset). Simply replacing them with the mean or the median age might not be the best solution since the age may differ by groups and categories of passengers.\n\nTo understand why, let's group our dataset by sex, Title and passenger class and for each subset compute the median age.\n\nTo avoid data leakage from the test set, we fill in missing ages in the train using the train set and we fill in ages in the test set using values calculated from the train set as well."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f9b51bf86204135ffb7e151d349a65e33677056b"
      },
      "cell_type": "code",
      "source": "# missing ages in train set\n\ncombined.iloc[:891].Age.isnull().sum()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e80279a9f4c4b2769afcc0382e2bc8cf60e5d7fd"
      },
      "cell_type": "code",
      "source": "# missing ages in test set\n\ncombined.iloc[891:].Age.isnull().sum()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "29109546482be71546101cc5a7e3893312f9afd0"
      },
      "cell_type": "code",
      "source": "grouped_train = combined.iloc[:891].groupby(['Sex','Pclass','Title'])\ngrouped_median_train = grouped_train.median()\ngrouped_median_train = grouped_median_train.reset_index()[['Sex', 'Pclass', 'Title', 'Age']]\n\ngrouped_median_train",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1e170c630e745c6c936a40b6833e49ad8cba78c4"
      },
      "cell_type": "code",
      "source": "# a function that fills in the missing age in combined based on these different attributes\n\ndef fill_age(row):\n    condition = (\n        (grouped_median_train['Sex'] == row['Sex']) & \n        (grouped_median_train['Title'] == row['Title']) & \n        (grouped_median_train['Pclass'] == row['Pclass'])\n    ) \n    return grouped_median_train[condition]['Age'].values[0]\n\n\ndef process_age():\n    global combined\n    # a function that fills the missing values of the Age variable\n    combined['Age'] = combined.apply(lambda row: fill_age(row) if np.isnan(row['Age']) else row['Age'], axis=1)\n    status('age')\n    return combined\n\ncombined = process_age()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3ded0bc26c3b77ea30b3653d3176557ef89c5f22"
      },
      "cell_type": "code",
      "source": "# Process the names\n\ndef process_names():\n    global combined\n    # we clean the Name variable\n    combined.drop('Name', axis=1, inplace=True)\n    \n    # encoding in dummy variable\n    titles_dummies = pd.get_dummies(combined['Title'], prefix='Title')\n    combined = pd.concat([combined, titles_dummies], axis=1)\n    \n    # removing the title variable\n    combined.drop('Title', axis=1, inplace=True)\n    \n    status('names')\n    return combined\n\n# This function drops the Name column since we won't be using it anymore because we created a Title column.\n# Then we encode the title values using a dummy encoding.\n# learn about dummy coding and how to easily do it in Pandas - http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html\n\ncombined = process_names()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "59ee96f278f0485f888383fbd4ea745cd9e10023"
      },
      "cell_type": "code",
      "source": "combined.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fbfb6dbdbe223a18c31133c3bf4c4772425098bb"
      },
      "cell_type": "code",
      "source": "# Impute the missing fare value by the average fare computed on the train set\n\ndef process_fares():\n    global combined\n    # there's one missing fare value - replacing it with the mean.\n    combined.Fare.fillna(combined.iloc[:891].Fare.mean(), inplace=True)\n    status('fare')\n    return combined\n\ncombined = process_fares()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d9c6c9441f4e2e0d1f44c6d4f797cc840eab2523"
      },
      "cell_type": "code",
      "source": "# fill the missing embarked value with the most frequent one in the train set which is S\n\ndef process_embarked():\n    global combined\n    combined.Embarked.fillna('S', inplace=True)\n    # dummy encoding \n    embarked_dummies = pd.get_dummies(combined['Embarked'], prefix='Embarked')\n    combined = pd.concat([combined, embarked_dummies], axis=1)\n    combined.drop('Embarked', axis=1, inplace=True)\n    status('embarked')\n    return combined\n\ncombined = process_embarked()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "17d34ae588e8f8c6e10f251da11afed0256f4d8e"
      },
      "cell_type": "code",
      "source": "combined.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "86a6558a8ef4c75584890f8c28302a4a4c99119d"
      },
      "cell_type": "code",
      "source": "train_cabin, test_cabin = set(), set()\n\nfor c in combined.iloc[:891]['Cabin']:\n    try:\n        train_cabin.add(c[0])\n    except:\n        train_cabin.add('U')\n        \nfor c in combined.iloc[891:]['Cabin']:\n    try:\n        test_cabin.add(c[0])\n    except:\n        test_cabin.add('U')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "26e630777114466c7ed52748874431b6a7db0edd"
      },
      "cell_type": "code",
      "source": "train_cabin",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f9388dde697bb725937da05518757bd53097bdf2"
      },
      "cell_type": "code",
      "source": "test_cabin",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9183ae7895ca55058ee9f0a353fb3b968a8727d6"
      },
      "cell_type": "code",
      "source": "# This function replaces NaN values with U (for Unknow). \n# It then maps each Cabin value to the first letter. \n# Then it encodes the cabin values using dummy encoding again.\n\ndef process_cabin():\n    global combined    \n    # replacing missing cabins with U (for Uknown)\n    combined.Cabin.fillna('U', inplace=True)\n    \n    # mapping each Cabin value with the cabin letter\n    combined['Cabin'] = combined['Cabin'].map(lambda c: c[0])\n    \n    # dummy encoding ...\n    cabin_dummies = pd.get_dummies(combined['Cabin'], prefix='Cabin')    \n    combined = pd.concat([combined, cabin_dummies], axis=1)\n\n    combined.drop('Cabin', axis=1, inplace=True)\n    status('cabin')\n    return combined\n\ncombined = process_cabin()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ee46ce2a8c9fba20953558268546937339675892"
      },
      "cell_type": "code",
      "source": "combined.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fa8146b27c43b9cc287ca17ffc5ba2ee5b102034"
      },
      "cell_type": "code",
      "source": "def process_sex():\n    global combined\n    # mapping string values to numerical one \n    combined['Sex'] = combined['Sex'].map({'male':1, 'female':0})\n    status('Sex')\n    return combined\n\ncombined = process_sex()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8930a8e31a008d1eb47e20630cfb500cdc23ae26"
      },
      "cell_type": "code",
      "source": "def process_pclass():\n    \n    global combined\n    # encoding into 3 categories:\n    pclass_dummies = pd.get_dummies(combined['Pclass'], prefix=\"Pclass\")\n    \n    # adding dummy variable\n    combined = pd.concat([combined, pclass_dummies],axis=1)\n    \n    # removing \"Pclass\"\n    combined.drop('Pclass',axis=1,inplace=True)\n    \n    status('Pclass')\n    return combined\n\ncombined = process_pclass()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a8da8836eb4c756f193fbcd59856a75b5258ac95"
      },
      "cell_type": "code",
      "source": "# Let's first see how many different ticket prefixes we have in our dataset\n\ndef cleanTicket(ticket):\n    ticket = ticket.replace('.', '')\n    ticket = ticket.replace('/', '')\n    ticket = ticket.split()\n    ticket = map(lambda t : t.strip(), ticket)\n    ticket = list(filter(lambda t : not t.isdigit(), ticket))\n    if len(ticket) > 0:\n        return ticket[0]\n    else: \n        return 'XXX'\n\n\ntickets = set()\nfor t in combined['Ticket']:\n    tickets.add(cleanTicket(t))\n\nlen(tickets)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "01fe95177276e07fb72e7ff3cfc6feb70e787780"
      },
      "cell_type": "code",
      "source": "def process_ticket():\n    \n    global combined\n    \n    # a function that extracts each prefix of the ticket, returns 'XXX' if no prefix (i.e the ticket is a digit)\n    def cleanTicket(ticket):\n        ticket = ticket.replace('.','')\n        ticket = ticket.replace('/','')\n        ticket = ticket.split()\n        ticket = map(lambda t : t.strip(), ticket)\n        ticket = list(filter(lambda t : not t.isdigit(), ticket))\n        if len(ticket) > 0:\n            return ticket[0]\n        else: \n            return 'XXX'\n    \n    # Extracting dummy variables from tickets:\n    combined['Ticket'] = combined['Ticket'].map(cleanTicket)\n    tickets_dummies = pd.get_dummies(combined['Ticket'], prefix='Ticket')\n    combined = pd.concat([combined, tickets_dummies], axis=1)\n    combined.drop('Ticket', inplace=True, axis=1)\n\n    status('Ticket')\n    return combined\n\ncombined = process_ticket()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "eeb03bdf8d7cb96ed4bad687c651407b20be69fe"
      },
      "cell_type": "code",
      "source": "# This part includes creating new variables based on the size of the family (the size is by the way, another variable we create).\n# This creation of new variables is done under a realistic assumption: \n# Large families are grouped together, hence they are more likely to get rescued than people traveling alone.\n\n# This function introduces 4 new features:\n#    FamilySize : the total number of relatives including the passenger.\n#    Sigleton : a boolean variable that describes families of size = 1\n#    SmallFamily : a boolean variable that describes families of 2 <= size <= 4\n#    LargeFamily : a boolean variable that describes families of 5 < size\n\ndef process_family():\n    \n    global combined\n    # introducing a new feature : the size of families (including the passenger)\n    combined['FamilySize'] = combined['Parch'] + combined['SibSp'] + 1\n    # introducing other features based on the family size\n    combined['Singleton'] = combined['FamilySize'].map(lambda s: 1 if s == 1 else 0)\n    combined['SmallFamily'] = combined['FamilySize'].map(lambda s: 1 if 2 <= s <= 4 else 0)\n    combined['LargeFamily'] = combined['FamilySize'].map(lambda s: 1 if 5 <= s else 0)\n    \n    status('family')\n    return combined\n\ncombined = process_family()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e4e8885469ffd11bb83ca70f61337216307d12f0"
      },
      "cell_type": "code",
      "source": "combined.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "34a07d15be1043e93d07c79f352163cd9c899cbf"
      },
      "cell_type": "code",
      "source": "combined.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fafb8bb0c4592e19014dbc09e9057179f4353eae"
      },
      "cell_type": "markdown",
      "source": "**Modeling**\n\nIn this part, we use our knowledge of the passengers based on the features we created and then build a statistical model. You can think of this model as a box that crunches the information of any new passenger and decides whether or not he survives.\n\nThere is a wide variety of models to use, from logistic regression to decision trees and more sophisticated ones such as random forests and gradient boosted trees.\n\nWe'll be using Random Forests. Random Froests has proven a great efficiency in Kaggle competitions.\n\nNow we have to:\n* Break the combined dataset in train set and test set.\n* Use the train set to build a predictive model.\n* Evaluate the model using the train set.\n* Test the model using the test set and generate and output file for the submission.\n\nKeep in mind that we'll have to reiterate on 2. and 3. until an acceptable evaluation score is achieved."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a4ec7a6f983640980d98107ff5382ffa36aa882e"
      },
      "cell_type": "code",
      "source": "# Let's start by importing the useful libraries.\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3ff22ee7664be9ab9114e0838802a04d26df4f6e"
      },
      "cell_type": "code",
      "source": "# To evaluate our model we'll be using a 5-fold cross validation with the accuracy since it's the metric that the competition uses in the leaderboard.\n# To do that, we'll define a small scoring function.\n\ndef compute_score(clf, X, y, scoring='accuracy'):\n    xval = cross_val_score(clf, X, y, cv = 5, scoring=scoring)\n    return np.mean(xval)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fc884450082c35577d321d1a5a231e2527ae954f"
      },
      "cell_type": "code",
      "source": "# Recovering the train set and the test set from the combined dataset is an easy task.\n\ndef recover_train_test_target():\n    global combined\n    \n    targets = pd.read_csv('../input/train.csv', usecols=['Survived'])['Survived'].values\n    train = combined.iloc[:891]\n    test = combined.iloc[891:]\n    \n    return train, test, targets\n\ntrain, test, targets = recover_train_test_target()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b9c4b28357c295ac04826d08537e60e3a5633e67"
      },
      "cell_type": "markdown",
      "source": "**Feature selection**\n\nWe've come up to more than 30 features so far. This number is quite large.\n\nWhen feature engineering is done, we usually tend to decrease the dimensionality by selecting the \"right\" number of features that capture the essential data.\n\nIn fact, feature selection comes with many benefits:\n* It decreases redundancy among the data\n* It speeds up the training process\n* It reduces overfitting\n\nTree-based estimators can be used to compute feature importances, which in turn can be used to discard irrelevant features.\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ca2601491bc6743dc2cb94fc0ac5e936b289200c"
      },
      "cell_type": "code",
      "source": "clf = RandomForestClassifier(n_estimators=50, max_features='sqrt')\nclf = clf.fit(train, targets)\n\nfeatures = pd.DataFrame()\nfeatures['feature'] = train.columns\nfeatures['importance'] = clf.feature_importances_\nfeatures.sort_values(by=['importance'], ascending=True, inplace=True)\nfeatures.set_index('feature', inplace=True)\n\nfeatures.plot(kind='barh', figsize=(25, 25))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "635e6861a92cb5c575f697aa9ed52bf075acf7ff"
      },
      "cell_type": "code",
      "source": "# Let's now transform our train set and test set in a more compact datasets.\n\nmodel = SelectFromModel(clf, prefit=True)\ntrain_reduced = model.transform(train)\nprint(train_reduced.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3c599e59ec29514c2e90b1ad670d14a2291cc3dc"
      },
      "cell_type": "code",
      "source": "test_reduced = model.transform(test)\nprint(test_reduced.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d40d15afb064a4a06041fc34f53a00cb398bbfcd"
      },
      "cell_type": "markdown",
      "source": "Now we're down to a lot less features.\n\nWe'll see if we'll use the reduced or the full version of the train set."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7ae8216a2be12c7fc83c7b818b76dc829f8fcd3a"
      },
      "cell_type": "code",
      "source": "# Let's try different base models\n\nlogreg = LogisticRegression()\nlogreg_cv = LogisticRegressionCV()\nrf = RandomForestClassifier()\ngboost = GradientBoostingClassifier()\n\nmodels = [logreg, logreg_cv, rf, gboost]\n\nfor model in models:\n    print('Cross-validation of : {0}'.format(model.__class__))\n    score = compute_score(clf=model, X=train_reduced, y=targets, scoring='accuracy')\n    print('CV score = {0}'.format(score))\n    print('****')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ad98cfcf0fe69324858bca0d6cce967fad1504ba"
      },
      "cell_type": "markdown",
      "source": "**Hyperparameters tuning**\n\nAs mentioned in the beginning of the Modeling part, we will be using a Random Forest model. It may not be the best model for this task but will show how to tune. This work can be applied to different models.\n\nRandom Forest are quite handy. They do however come with some parameters to tweak in order to get an optimal model for the prediction task.\n\nTo learn more about Random Forests, you can refer to this link : https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/\n\nAdditionally, we'll use the full train set."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a5928248545108d0e1410625b4f6d6d2b405343c"
      },
      "cell_type": "code",
      "source": "# turn run_gs to True if you want to run the gridsearch again.\nrun_gs = False\n\nif run_gs:\n    parameter_grid = {\n                 'max_depth' : [4, 6, 8],\n                 'n_estimators': [50, 10],\n                 'max_features': ['sqrt', 'auto', 'log2'],\n                 'min_samples_split': [2, 3, 10],\n                 'min_samples_leaf': [1, 3, 10],\n                 'bootstrap': [True, False],\n                 }\n    forest = RandomForestClassifier()\n    cross_validation = StratifiedKFold(n_splits=5)\n\n    grid_search = GridSearchCV(forest,\n                               scoring='accuracy',\n                               param_grid=parameter_grid,\n                               cv=cross_validation,\n                               verbose=1\n                              )\n\n    grid_search.fit(train, targets)\n    model = grid_search\n    parameters = grid_search.best_params_\n\n    print('Best score: {}'.format(grid_search.best_score_))\n    print('Best parameters: {}'.format(grid_search.best_params_))\n    \nelse: \n    parameters = {'bootstrap': False, 'min_samples_leaf': 3, 'n_estimators': 50, \n                  'min_samples_split': 10, 'max_features': 'sqrt', 'max_depth': 6}\n    \n    model = RandomForestClassifier(**parameters)\n    model.fit(train, targets)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "757d1b4973c260db53566de34cbfa9b492024c12"
      },
      "cell_type": "code",
      "source": "# Now that the model is built by scanning several combinations of the hyperparameters, we can generate an output file to submit on Kaggle.\noutput = model.predict(test).astype(int)\ndf_output = pd.DataFrame()\naux = pd.read_csv('../input/test.csv')\ndf_output['PassengerId'] = aux['PassengerId']\ndf_output['Survived'] = output\ndf_output[['PassengerId','Survived']].to_csv('gridsearch_rf.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bf85554397802f0d477e5a22782baa9079203066"
      },
      "cell_type": "code",
      "source": "!ls",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4305a689e025b779a4f1f7eb33e6690166257b32"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
